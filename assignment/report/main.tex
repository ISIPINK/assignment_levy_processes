\documentclass[a4paper,11pt]{article}

\setlength{\textwidth}{15.0cm}
\setlength{\textheight}{24.0cm}
\setlength{\topmargin}{0cm}
\setlength{\headsep}{0cm}
\setlength{\headheight}{0cm}
\setlength{\parindent}{0pt}
\pagestyle{plain}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, mathtools, amsthm, amssymb}
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cancel}

\allowdisplaybreaks
\input{environments.tex}

\usepackage[style=authoryear,backend=biber]{biblatex}

\addbibresource{bibliography.bib} 

\title{Assignment Levy processes}

\author{}
\date{}


\begin{document}

\maketitle

\begin{abstract}
    We elaborate some results from chapter $25$ of \citeauthor{sato_levy_2013}'s \citetitle{sato_levy_2013} \cite{sato_levy_2013}. Chapter $25$ of \cite{sato_levy_2013} discusses properties of $g$-moments of Lévy processes and it relation with the Lévy measure.
\end{abstract}

\section{Proof of Existence of Moments from Smoothness of Characteristic Function}

In this section we prove the partial reverse of the existence of moments implies smoothness of the characteristic function.

\begin{definition}[$\Delta^{k}_{h}$]
    We define $\forall h>0: \Delta_{h}$ ($h$-central difference) of $f:\mathbb{R}\rightarrow \mathbb{R}$ as follows:
    \begin{equation}
        \Delta_{h} f(t) = f(t+h)- f(t-h),
    \end{equation}
    and $\forall k \in \mathbb{N}:\Delta^{k}_{h}$ is defined as applying $\Delta_{h}$ applying $k$ times and
    $k=0$ as the identity. Note that $\Delta^{k}_{h}$ is a linear operator. We will always take
    differences in respect to the $t$ variable.
\end{definition}

\begin{lemma}[$\Delta^{k}_{h} e^{at}$]
    \begin{equation}
        \forall k \in \mathbb{N}: \Delta^{k}_{h} e^{at} =  e^{at} \left( e^{ah} - e^{-ah} \right)^{k}.
    \end{equation}
\end{lemma}

\begin{proof}
    This follows simply by induction. $k=0$ is trivial.
    Base case:
    \begin{align}
        \Delta_{h} e^{at} & =\left( e^{a(t+h)} - e^{a(t-h)} \right) \\
                          & =e^{at}\left( e^{ah} - e^{-ah} \right)
        .
    \end{align}
    Induction hypothesis:
    \begin{equation}
        \Delta^{k-1}_{h} e^{at} = e^{at} \left( e^{ah} - e^{-ah} \right)^{k-1}.
    \end{equation}

    Induction step:
    \begin{align}
        \Delta^{k}_{h} e^{at} & = \Delta_{h} \Delta^{k-1}_{h} (e^{at})                                        \\
                              & = \Delta_{h} \left( e^{at} \left( e^{ah} - e^{-ah} \right)^{k-1} \right)      \\
                              & = \Delta_{h} \left( e^{at}\right) \left( e^{ah} - e^{-ah} \right)^{k-1}       \\
                              & = e^{at}\left( e^{ah} - e^{-ah} \right) \left( e^{ah} - e^{-ah} \right)^{k-1} \\
                              & =  e^{at} \left( e^{ah} - e^{-ah} \right)^{k}.
    \end{align}
\end{proof}


\begin{theorem}[central finite difference formula]
    $\forall k \in \mathbb{N}$ $\forall$  $k$-continuously differentiable $f$'s at $0$:
    \begin{equation}
        f^{(k)}(0) = \lim_{h \to 0} \frac{\Delta^{k}_{h} f(t)|_{t=0}}{(2h)^{k}}.
    \end{equation}
\end{theorem}

\begin{proof}
    Follows by the mean value theorem.
    \begin{align}
         & \lim_{h \to 0} \frac{\Delta^{k}_{h} f(t)|_{t=0}}{(2h)^{k}}                                                                              \\
         & = \lim_{h \to 0} \frac{\Delta_{h}\Delta^{k-1}_{h} f(t)|_{t=0}}{(2h)^{k}}                                                                \\
         & = \lim_{h \to 0} \frac{\Delta^{k-1}_{h} f(t)(|_{t=h}-|_{t=-h})}{(2h)^{k}}                                                               \\
         & = \lim_{h \to 0} \frac{2h (\Delta^{k-1}_{h} f(t))'|_{t=z^{1}_{h}}}{(2h)^{k}}, z^{1}_{h} \in[-h,h]                                       \\
         & = \lim_{h \to 0} \frac{\Delta^{k-1}_{h} f'(t)|_{t=z^{1}_{h}}}{(2h)^{k-1}}, z^{1}_{h} \in[-h,h]                                          \\
         & = \lim_{h \to 0} \frac{\Delta^{k-2}_{h} f^{(2)}(t)|_{t=z^{2}_{h}}}{(2h)^{k-2}}, z^{2}_{h} \in[z^{1}_{h}-h,z^{1}_{h}+h] \subset [-2h,2h] \\
         & \quad ...                                                                                                                               \\
         & = \lim_{h \to 0} f^{(k)}(t)|_{t=z^{k}_{h}},  z^{k}_{h} \in [-k h, k h]                                                                  \\
         & = \lim_{h \to 0} f^{(k)}(z^{k}_{h}) = f^{(k)}(0).
    \end{align}
    Last line follows because $f^{k}$ is continuous at $0$. This proof may need some modification because we need to assume that there exist a neighborhood around $0$ where $f$ is $k$-times continuously differentiable instead only $k$-times continuously differentiable at $0$.
\end{proof}


\begin{lemma} \label{lemma:moment_smooth_char}
    $\forall k \in  \mathbb{N}:\phi_{X}(t)$ is $2k$-time continuous differentiable $\Rightarrow  E[X^{2k}] < \infty$
\end{lemma}

\begin{proof}
    Use the central finite difference formula and previous lemma:
    \begin{align}
        |\phi_{X}^{(2k)}(0)| & = \left|\lim_{h \to 0} \frac{\Delta^{2k}_{h}|_{t=0} \phi_{X}(t)}{(2h)^{2k}}\right|                      \\
                             & = \left|\lim_{h \to 0} \frac{\Delta^{2k}_{h}|_{t=0} E\left[e^{itX}\right]}{(2h)^{2k}}\right|            \\
                             & = \left|\lim_{h \to 0} \frac{ E\left[\Delta^{2k}_{h}|_{t=0}e^{itX}\right]}{(2h)^{2k}}\right|            \\
                             & = \left|\lim_{h \to 0} \frac{ E\left[e^{itX} |_{t=0} (e^{ihX}-e^{-ihX})^{2k}\right]}{(2h)^{2k}}\right|  \\
                             & = \left|\lim_{h \to 0} \frac{ E\left[(e^{ihX}-e^{-ihX})^{2k}\right]}{(2h)^{2k}}\right|                  \\
                             & = \left|\lim_{h \to 0} \frac{1}{i^{2k}}E\left[\frac{ (e^{ihX}-e^{-ihX})^{2k}}{(2h)^{2k}} \right]\right| \\
                             & = \lim_{h \to 0} E\left[\frac{ \sin(Xh)^{2k}}{h^{2k}}\right]                                            \\
                             & = \lim_{h \to 0} E\left[ X^{2k}\frac{ \sin(Xh)^{2k}}{X^{2k} h^{2k}}\right]                              \\
                             & = \lim_{h \to 0} E\left[ X^{2k} \operatorname{sinc}(Xh) ^{2k}\right]                                    \\
                             & \ge  E\left[ X^{2k} \right]
    \end{align}
    Where last line follows by Fatou's lemma.
\end{proof}

\section{Submultiplicative Functions}
In this section we introduce submultiplicative functions and prove some properties of them.

\begin{definition}[submultiplicative function]
    A function $g(x)$ on $\mathbb{R}^{d}$ is submultiplicative if and only if it is nonnegative
    and $\exists a \in \mathbb{R}>0:$

    \begin{equation}
        g(x+y) \leq a g(x) g(y) \text { for } x, y \in \mathbb{R}^{d}
        .
    \end{equation}

\end{definition}

\begin{lemma}[induction on submultiplicativity]
    Let $g(x)$ be a submultiplicative with constant $a$ then:
    \begin{equation}
        \forall (x_{i})_{i} \subset \mathbb{R}^d:
        g\left(\sum_{i=1}^{n} x_{i}\right) \le a^{n-1} \prod_{i=1}^{n} g(x_{i})  .
    \end{equation}
\end{lemma}

\begin{proof}
    This follows by induction on $n$. \\
    Base case: $n=2$ is true by submultiplicativity. \\
    Induction hypothesis: assume $2 \le  n \le k$. \\
    Induction step: $n=k+1$:
    \begin{align}
        \forall (x_{i})_{i}                  & \subset \mathbb{R}^d:                                                                          \\
        g\left(\sum_{i=1}^{k+1} x_{i}\right) & = g\left(\sum_{i=1}^{k} x_{i} + x_{k+1}\right)                                                 \\
                                             & \le a g\left(\sum_{i=1}^{k} x_{i}\right) g(x_{k+1})  \text{ by submultiplicativity}            \\
                                             & \le a a^{k-1} \prod_{i=1}^{k}  g\left(x_{i}\right) g(x_{k+1})  \text{ by induction hypothesis} \\
                                             & \le a^{k} \prod_{i=1}^{k+1}  g\left(x_{i}\right) .
    \end{align}
\end{proof}

\begin{lemma}
    The product of two submultiplicative functions is submultiplicative.
\end{lemma}

\begin{proof}
    Let $g,h:\mathbb{R}^{d}\rightarrow \mathbb{R}$ be submultiplicative functions then:
    $$
        \forall x,y \in \mathbb{R}^{d}: g(x+y)h(x+y) \le a_{1} g(x)g(y) a_{2} h(x)h(y) \leq a_{1}a_{2} g(x)h(x)g(y)h(y).
    $$
    So $gh:\mathbb{R}^{d}\rightarrow \mathbb{R}$ is submultiplicative.
\end{proof}

\begin{lemma}
    If $g(x)$ is submultiplicative on $\mathbb{R}^{d}$, then so is $g(c x+\gamma)^{\alpha}$
    with $c \in \mathbb{R}$, $\gamma \in \mathbb{R}^{d}$, and $\alpha>0$.
\end{lemma}

\begin{proof}
    Let $g_{1}(x)=g(c x), g_{2}(x)=g(x+\gamma)$, and $g_{3}(x)=g(x)^{\alpha}$.
    \begin{align}
        \forall x,y & \in \mathbb{R}^{d}:                                                                                                                 \\
        g_{1}(x+y)  & = g(c(x+y)) = g(c x + c y) \leq a g(c x) g(c y) = a g_{1}(x) g_{1}(y)                                                               \\
        g_{2}(x+y)  & = g(x + y + \gamma) = g(x + \gamma + y + \gamma - \gamma)
        \le a^{2} g(-\gamma) g(x+\gamma) g(y + \gamma)                                                                                                    \\
                    & \le a^{2} g(-\gamma) g_{2}( x)g_{2}(y)                                                                                              \\
        g_{3}(x+y)  & = g(x+y) ^{\alpha} \le \left(a g(x) g(y) \right) ^{\alpha}= a^{\alpha} g(x)^{\alpha} g(y)^{\alpha} = a^{\alpha} g_{3}(x) g_{3}(y) .
    \end{align}
\end{proof}

\begin{example}[submultiplicative functions]
    The class of submultiplicative functions is broad. Let $0<\beta \leq 1$. Then the following functions are submultiplicative:
    \begin{gather}
        |x| \vee 1, \left|x_{j}\right| \vee 1, x_{j} \vee 1, \exp \left(|x|^{\beta}\right), \exp \left(\left|x_{j}\right|^{\beta}\right) \\
        \exp \left(\left(x_{j} \vee 0\right)^{\beta}\right), \log (|x| \vee e), \log \left(\left|x_{j}\right| \vee e\right), \log \left(x_{j} \vee \mathrm{e}\right) \\
        \log \log \left(|x| \vee e^{e}\right), \log \log \left(\left|x_{j}\right| \vee e^{e}\right), \log \log \left(x_{j} \vee e^{e}\right)
    \end{gather}
\end{example}

\begin{proof}
    See \cite{sato_levy_2013} page 159 PROPOSITION 25.4 (iii).
\end{proof}

\section{Finiteness of the $g$-moments of a L\'{e}vy process}

In this section we characterize the finiteness of the $g$-moment of a Lévy process in terms of the Lévy measure.

\begin{definition}[$g$-moment]
    Let $g(x)$ be a nonnegative measurable function on $\mathbb{R}^{d}$. We call
    $\int g(x) \mu(\mathrm{d} x)$ the $g$-moment of a measure $\mu$ on $\mathbb{R}^{d}$. We call $E[g(X)]$ the $g$-moment of a random variable $X$ on $\mathbb{R}^{d}$.
\end{definition}

\begin{definition}[locally bounded function]
    A function is locally bounded if and only if it is bounded on every compact set.
\end{definition}

The main result of the assignment is following theorem.


\begin{theorem}[charaterization of finiteness of the $g$-moment of a L\'{e}vy process] \label{th:char finite moment}
    Let $g$ be a submultiplicative, locally bounded, measurable
    function on $\mathbb{R}^{d}$ and let $\left\{X_{t}\right\}$ be a Lévy process
    on $\mathbb{R}^{d}$ with Lévy measure $\nu$. Then, $X_{t}$ has finite $g$-moment for every $t>0$ if
    and only if $[\nu]_{\{|x|>1\}}$ has finite $g$-moment.
\end{theorem}

\begin{corollary}[time independent property]
    Let $g$ be a submultiplicative, locally bounded, measurable
    function on $\mathbb{R}^{d}$. Then, finiteness of the $g$-moment is not a time dependent
    distributional property in the class of Lévy processes.
\end{corollary}
\begin{proof}
    Follows directly from Theorem \ref{th:char finite moment}.
\end{proof}

We prove Theorem \ref{th:char finite moment} after three lemmas.

\begin{lemma}[exponential bound for submultiplicative functions]
    If $g(x)$ is submultiplicative and locally bounded, then

    \begin{equation}
        g(x) \leq b e^{c|x|}
    \end{equation}
    with some $b>0$ and $c>0$.
\end{lemma}

\begin{proof}
    $\sup _{|x| \leq 1} g(x) < \infty$ because $g$ is locally bounded, let $b = \max(\sup _{|x| \leq 1} g(x),\frac{1}{a}+1)$ such that $\sup _{|x| \leq 1} g(x)\le b$ and $ab>1$.
    $\forall x$ set $n_x = \lceil \left| x \right| \rceil$ implying $n_x-1 < \left| x \right| \leq n_x, |\frac{x}{n_x}|\le 1$ and $g(\frac{x}{n _{x}})\le b$, then
    \begin{equation}
        g(x) = g\left(\sum_{i=1}^{n_x} \frac{x}{n_x}\right)
        \leq a^{n_x-1} g\left(\frac{x}{n_x} \right)^{n_x} \leq a^{n_x-1} b^{n_x} \leq b(a b)^{|x|}
        \le b e^{\ln \left(ab\right)|x|}.
    \end{equation}
\end{proof}

\begin{lemma}\label{lem:supp entire}
    Let $\mu$ be an infinitely divisible distribution on $\mathbb{R} $ with Lévy measure $\nu$
    supported on a bounded set $S\subset[-a, a]$ for some $a>0 \in \mathbb{R}$.
    Then $\widehat{\mu}(t)$ can be extended to an entire function on $\mathbb{C}$.
\end{lemma}

\begin{proof}
    The Lévy representation of $\widehat{\mu}(t)$ is
    \begin{align}
        \widehat{\mu}(t) & =\exp \left[-\frac{1}{2}At^{2} +\int_{[-a, a]}\left(\mathrm{e}^{i tx}-1-itx\right) \nu(dx)+i \gamma't \right]                  \\
                         & =\exp \left[-\frac{1}{2}At^{2}+i \gamma't \right] \exp \left[\int_{-a}^{a}\left(\mathrm{e}^{i tx}-1-itx\right) \nu(dx) \right]
    \end{align}

    with some $\gamma^{\prime} \in \mathbb{R}$.

    As products, sums, compositions and integrals of entire functions are entire (integrals follow from Morera's theorem), $\exp$ and polynomials are entire it is sufficient to show that $\int_{-a}^{a}\left(\mathrm{e}^{i tx}-1-itx\right) \nu(dx)$ is finite for $t \in \mathbb{C}$:

    \begin{align}
         & \left|\int_{-a}^{a}\left(\mathrm{e}^{i tx}-1-itx\right) \nu(dx) \right|                                                                                                 \\
         & \leq \int_{-a}^{a}|\mathrm{e}^{i tx}-1-itx| \nu(dx)                                                                                                                     \\
         & \leq \int_{-1}^{1}|\mathrm{e}^{i tx}-1-itx| \nu(dx) + \int_{[-a,a] \setminus [-1,1]}|\mathrm{e}^{i tx}-1-itx| \nu(dx)                                                   \\
         & \leq \int_{-1}^{1}|\mathrm{e}^{i tx}-1-itx| \nu(dx) + \sup_{x \in [-a,a] \setminus [-1,1]}\left(|\mathrm{e}^{i tx}-1-itx|\right) \int_{[-a,a] \setminus [-1,1]} \nu(dx) \\
         & \leq \int_{-1}^{1}|\mathrm{e}^{i tx}-1-itx| \nu(dx) + I_{2}                                                                                                             \\
         & \leq \int_{-1}^{1}\left|\sum_{n=2}^{\infty} \frac{(ixt)^{n}}{n!}\right| \nu(dx) + I_{2}                                                                                 \\
         & \leq \int_{-1}^{1}\sum_{n=2}^{\infty} \frac{|x|^{n}|t|^{n}  }{n!} \nu(dx) + I_{2}                                                                                       \\
         & \leq \int_{-1}^{1}|x|^{2} \sum_{n=2}^{\infty} \frac{|x|^{n-2} |t| ^{n}}{n!} \nu(dx) + I_{2}                                                                             \\
         & \leq \int_{-1}^{1}|x|^{2} \sum_{n=2}^{\infty} \frac{|t|^{n}}{n!} \nu(dx) + I_{2}                                                                                        \\
         & \leq  \sum_{n=2}^{\infty} \frac{|t|^{n}}{n!} \int_{-1}^{1}|x|^{2} \nu(dx) + I_{2}                                                                                       \\
         & \leq  (e^{|t| }-|t| -1)  \int_{-1}^{1}|x|^{2} \nu(dx) + I_{2}                                                                                                           \\
         & < \infty
        .
    \end{align}
    We implicitly used $|x|<1 \implies |x|^{n-2}<1$,$|\mathrm{e}^{i tx}-1-itx|$ continuous in $x$ so achieves maximum on compact sets (Weierstrass theorem) and

    \begin{equation}
        \int_{\mathbb{R}} (|x|^{2} \wedge 1) \nu(dx)<\infty \implies \int_{-1}^{1} |x|^{2} \nu(dx)<\infty, \int_{[-a,a] \setminus [-1,1]} \nu(dx)<\infty   .
    \end{equation}

\end{proof}


\begin{lemma}[entire characteristic functions imply finite exponential moments] \label{lem:entire_char_mom}
    If $\mu$ is a probability measure on $\mathbb{R}$ and $\widehat{\mu}(z)$ is extendible to an entire
    function on $\mathbb{C}$, then $\mu$ has finite exponential moments, that is, it has finite $e^{c|x|}$-moment for every $c>0$.
\end{lemma}

\begin{proof}
    As entire functions are infinitely differentiable it follows from Lemma \ref{lemma:moment_smooth_char} that $\alpha_{2n}=\int x^{2n} \mu(\mathrm{d} x) =\int |x|^{2n} \mu(\mathrm{d} x) $ are finite for any $n \geq 1$ and by H\"{o}lder's inequality the uneven (absolute) moments $(\beta_{2n+1}),\alpha_{2n+1}$ are also finite.
    Since $\frac{\mathrm{d}^{n} \widehat{\mu}}{\mathrm{d} z^{n}}(0)=\mathrm{i}^{n} \alpha_{n}$, we have

    $$
        \widehat{\mu}(z)=\sum_{n=0}^{\infty} \frac{1}{n !} \mathrm{i}^{n} \alpha_{n} z^{n}
    $$

    as $\widehat{\mu}(z)$ is entire the radius of convergence of the right-hand side is infinite therefore $ \forall z \in \mathbb{C}: \left| \frac{z^{n} \alpha_{n}}{n!} \right| = \frac{|z|^{n} }{n!} \alpha_{n} \rightarrow 0$ and again by H\"{o}lder's inequality also  $\forall c \in \mathbb{R}:\frac{2^{n}c^{n}}{n!} \beta_{n}\rightarrow 0 \implies \forall \delta>0 \in \mathbb{R}, \exists n _{c} \in  \mathbb{N},  \forall n>n _{\delta} \in \mathbb{N}:\frac{2^{n}c^{n}}{n!} \beta_{n} <\delta$. Now the exponential moments are easily bounded:

    \begin{align}
        \int \mathrm{e}^{c|x|} \mu(\mathrm{d} x) & =  \int \sum_{n=0}^{\infty} \frac{c^{n} |x|^{n} }{n!} \mu(\mathrm{d} x)      \\
                                                 & =  \sum_{n=0}^{\infty}\frac{c^{n} }{n!} \int |x|^{n} \mu(\mathrm{d} x)       \\
                                                 & \le \sum_{n=0}^{\infty} \frac{1}{n !} \beta_{n} c^{n}                        \\
                                                 &
        \le\sum_{n=0}^{n _{\delta}} \frac{1}{n !} \beta_{n} c^{n} + \sum_{n=n _{\delta}}^{\infty} \frac{1}{n !} \beta_{n} c^{n} \\
                                                 &
        \le\sum_{n=0}^{n _{\delta}} \frac{1}{n !} \beta_{n} c^{n} + \sum_{n=n _{\delta}}^{\infty} \frac{\delta}{2^{n}}          \\
                                                 & < \infty.
    \end{align}
    Exchanging the sum and the integral is justified by Tonelli's theorem as the summand is positive.
\end{proof}

\begin{corollary}
    A L\'{e}vy process with a L\'{e}vy measure supported on a bounded set has finite exponential moments.
\end{corollary}
\begin{proof}
    Follows from Lemma's \ref{lemma:moment_smooth_char}, \ref{lem:supp entire}.
\end{proof}

\begin{proof}[Proof of Theorem 25.3]

    Let $\nu_{0}=[\nu]_{\{|x| \leq 1\}}$ and $\nu_{1}=[\nu]_{\{|x|>1\}}$. Construct
    independent Lévy processes $\left\{X_{t}^{0}\right\}$ and $\left\{X_{t}^{1}\right\}$ on $\mathbb{R}^{d}$ such
    that $\left\{X_{t}\right\} =_{d} \left\{X_{t}^{0}+ X_{t}^{1}\right\}$ and
    $\left\{X_{t}^{1}\right\}$ is compound Poisson with Lévy measure $\nu_{1}$. Let $\mu_{0}^{\textcolor{blue}{t}}$ and $\mu_{1}^{\textcolor{blue}{t}}$ be
    the distributions of $X_{\textcolor{blue}{t}}^{0}$ and $X_{\textcolor{blue}{t}}^{1}$, respectively.

    Suppose that $X_{t}$ has finite $g$-moment for some $t>0$. It follows from

    $$
        \textcolor{blue}{\infty>}E\left[g\left(X_{t}\right)\right]
        = \textcolor{blue}{ E\left[g\left(X^{0}_{t} + X^{1}_{t}\right)\right]}
        = \iint g(x+y) \mu_{0}^{t}(dx) \mu_{1}^{t}(dy)
    $$

    that $\int g(x+y) \mu_{1}^{t}(dy)<\infty$ for some $x$. \textcolor{red}{This means}  \textcolor{blue}{ Because $\mu_{1}^{t}$ is a compound Poisson we have (see (24.1) condition on the Poisson distribution of the compound Poisson process)}

    $$
        \sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g(x+y) \nu_{1}^{n}(dy)<\infty.
    $$

    Since $g(y) \leq a g(-x) g(x+y) \leq a b e^{c|x|} g(x+y)$ by Lemma 25.5, we get


    \begin{equation*}
        \sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g(y) \nu_{1}^{n}(dy)<\infty. \tag{25.4}
    \end{equation*}

    Hence $\int g(y) \nu_{1}(d y)<\infty$.

    Conversely, suppose that $\int g(y) \nu_{1}(dy) \textcolor{blue}{= G}<\infty$. Let us prove
    that $E\left[g\left(X_{t}\right)\right]$ $<\infty$ for every $t$. By the submultiplicativity,
    \begin{align*}
        E\left[g\left(X_{t}\right)\right] & =  \textcolor{blue}{E\left[g\left(X_{t}^{0} + X_{t}^{1}\right)\right]}                                    \\
                                          & \leq \textcolor{blue}{aE\left[ g\left(X_{t}^{0}\right) g\left(X_{t}^{1}\right)\right]}                    \\
                                          & = \textcolor{blue}{aE\left[ g\left(X_{t}^{0}\right)\right]  E\left[ g\left(X_{t}^{1}\right)\right]}       \\
                                          & \le a b E\left[\mathrm{e}^{\mathrm{c}\left|X_{t}^{0}\right|}\right] E\left[g\left(X_{t}^{1}\right)\right]
    \end{align*}

    \textcolor{blue}{So it is sufficient to show that $E\left[g\left(X_{t}^{1}\right) \right]<\infty$ and
    $E\left[\mathrm{e}^{\mathrm{c}\left|X_{t}^{0}\right|}\right] < \infty$
    \begin{align*}
        E[g(X^{1}_{t})] & = \int g(x) \mu _{1} ^{t}(dx)                                                                                            \\
                        & =\sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g(x) \nu_{1}^{n}(dx)                                                         \\
                        & =\sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g\left(\sum_{j} x_{j}\right) \prod_{j}\nu_{1}\left(dx_{j}\right)             \\
                        & \le \sum_{n=0}^{\infty} \frac{t^{n}}{n !} a^{n-1}\int \prod_{j} g\left(x_{j}\right) \prod_{j} \nu_{1}\left(dx_{j}\right) \\
                        & =\sum_{n=0}^{\infty} \frac{t^{n}}{n !} a^{n-1} \left(\int g(x) \nu_{1}(dx)\right)^{n}                                    \\
                        & =a^{-1}\sum_{n=0}^{\infty} \frac{t^{n}}{n !} a^{n} G^{n}                                                                 \\
                        & = a^{-1} e^{aGt} < \infty.
    \end{align*}
    }
    \textcolor{red}{
    $$
        \begin{aligned}
            \int g(y) \nu_{1}^{n}(dy) & =\int \cdots \int g\left(y_{1}+\cdots+y_{n}\right) \nu_{1}\left(dy_{1}\right) \ldots \nu_{1}\left(d y_{n}\right) \\
                                      & \leq a^{n-1}\left(\int g(y) \nu_{1}(dy)\right)^{n}
        \end{aligned}
    $$
    Hence we have (25.4) for every $t$. That is, $X_{t}^{1}$ has finite $g$-moment.
    Since by (25.1) and (25.3), it remains only to show that $E\left[e^{c |X_{t}^{0}| }\right]<\infty$.
    }

    Let $X_{j}^{0}(t)$, $1 \leq j \leq d$, be the components of $X_{t}^{0}$. Then

    \begin{align*}
        E\left[e^{c\left|X_{t}^{0}\right|}\right] & \leq E\left[\exp \left(c \sum_{j=1}^{d}\left|X_{j}^{0}(t)\right|\right)\right] \leq E\left[\prod_{j=1}^{d}\left(e^{c X_{j}^{0}(t)}+e^{-c X_{j}^{0}(t)}\right)\right] \\
                                                  & \textcolor{blue}{
            \le  E \left[ \sum_{\alpha_{j} \in \{-1,1\}} e^{c \sum_{ j=1} ^{d} \alpha_{j} X^{0}_{j}(t)} \right]
            \le  \sum_{\alpha_{j} \in \{-1,1\}} E \left[e^{c \sum_{ j=1} ^{d} \alpha_{j} X^{0}_{j}(t)} \right]
        }
    \end{align*}

    \textcolor{red}{
        which is written as a sum of a finite number of terms of the form $E\left[\exp X_{t}^{\sharp}\right]$
        with $X_{t}^{\sharp}$ being a linear combination of $X_{j}^{0}(t), 1 \leq j \leq d$. Since $\left\{X_{t}^{\sharp}\right\}$
        is a Lévy process on $\mathbb{R}$ with Lévy measure supported on a bounded set (use Proposition 11.10),
        $E\left[\exp X_{t}^{\sharp}\right]$ is finite by virtue of Lemmas 25.6 and 25.7. This proves all statements in the theorem.\\
    }
    \textcolor{blue}{
    Since $X^{0}_{t}$ is a Lévy process with a Lévy measure supported on a bounded set,
    by proposition 11.10 is $\sum_{j=1}^{d} \alpha_{j} X^{0}_{j}(t)$ a Lévy process on $\mathbb{R}$ with a Lévy measure supported on a bounded set
    therefore it has finite (exponential) moments by lemma 25.7. I.e. $E\left[e^{c\left|X_{t}^{0}\right|}\right] \le  E \left[e^{c \sum_{ j=1} ^{d} \alpha_{j} X^{0}_{j}(t)} \right] <  \infty$.
    }
\end{proof}

Corollary 25.8. Let $\alpha>0,0<\beta \leq 1$, and $\gamma \geq 0$. None of the properties
$\int|x|^{\alpha} \mu(\mathrm{d} x)<\infty, \int(0 \vee \log |x|)^{\alpha} \mu(\mathrm{d} x)<\infty$, and
$\int|x|^{\gamma} e^{\alpha|x|^{\beta}} \mu(\mathrm{d} x)<$ $\infty$ is time dependent in the class
of Lévy processes. For a Lévy process on $\mathbb{R}^{d}$ with Lévy measure $\nu$, each of the properties
is expressed by the corresponding property of $[\nu]_{\{|x|>1\}}$.

\begin{proof}
    This follows from Theorem 25.3 and Proposition 25.4.
\end{proof}


EXAMPLE 25.12. Let $\left\{X_{t}\right\}$ be a Lévy process on $\mathbb{R}^{d}$ generated
by $(A, \nu, \gamma )$. In components, $X_{t}=\left(X_{j}(t)\right), \gamma=\left(\gamma_{j}\right)$,
and $A=\left(A_{j k}\right)$. Then $X_{t}$ has finite mean for $t>0$
if and only if $\int_{\{|x| >1\}} |x| \nu(\mathrm{d} x)<\infty$.
When this condition is met, we can find $m_{j}(t)=E\left[X_{j}(t)\right]$ expressed as


\begin{equation*}
    m_{j}(t)=t\left(\int_{|x|>1} x_{j} \nu(\mathrm{d} x)+\gamma_{j}\right)=t \gamma_{1, j}, \quad j=1, \ldots, d \tag{25.7}
\end{equation*}

differentiating $\widehat{\mu}(z)$ (Proposition 2.5(ix) , \textcolor{blue}{$m_{j}(t) = \frac{1}{i} \partial z_{j}|_{z=0} \widehat{\mu_{t}}(z)$}).

\textcolor{blue}{
\small
\begin{align*}
    \widehat{\mu}_{t}(z)                                                      =
    \operatorname{e x p} \biggl[-t                                           & \left(\frac{1} {2} \langle z, A z \rangle +i \langle\gamma, z \rangle+\int_{\mathbb{R}^{d}} \bigl( \operatorname{e x p} ( i \langle z, x \rangle)-1-i \langle z, x \rangle I_{D} ( x ) \bigr) \, \nu( d x )\right) \biggr], \\
    \partial z_{j}|_{z=0}\left( -\frac{1} {2} \langle z, A z \rangle \right) & =
    -\frac{1} {2} \langle \partial z_{j}(z), A z|_{z=0} \rangle
    -\frac{1} {2} \langle z|_{z=0}, A \partial z_{j}(z) \rangle=0,                                                                                                                                                                                                                                         \\
    \partial z_{j}|_{z=0}\left( i \langle \gamma, z \rangle \right)          & = i \langle\gamma, \partial z_{j}|_{z=0}(z) \rangle =
    i \langle\gamma, e_{j}  \rangle = i \gamma_{j}                           ,                                                                                                                                                                                                                             \\
    \partial z_{j}|_{z=0}                                                    & \left( \int_{\mathbb{R}^{d}}                         \bigl( \operatorname{e x p} ( i \langle z, x \rangle)-1-i \langle z, x \rangle I_{D} ( x ) \bigr) \nu(dx) \right)                                                      \\
                                                                             & = \int_{\mathbb{R}^{d}}                         \bigl( ix_{j}-i x_{j}I_{D} ( x ) \bigr) \nu(dx)                                                                                                                             \\
                                                                             & = i\int_{D^{c}} x_{j} \nu(dx)
\end{align*}
The differentiation of the integral is calculated by using the definition of the partial derivative, L'H\^{o}pital's rule, the dominated convergence theorem and $\int(|x|^{2} \wedge 1 ) \nu(dx) < \infty $.
}

Here $\gamma_{1, j}$ is the $j$ th
component of the center $\gamma_{1}$ in (8.8). Similarly, $E\left[\left|X_{t}\right|^{2}\right]<\infty$
for all $t>0$ if and only if $\int_{|x|>1}|x|^{2} \nu(\mathrm{d} x)<\infty$. In this case,

{ \small
        \begin{align*}
            v_{j k}(t) & =E\left[\left(X_{j}(t)-m_{j}(t)\right)\left(X_{k}(t)-m_{k}(t)\right)\right], \quad j, k=1, \ldots, d                                                                                                                                                                                                                                      \\
                       & = \textcolor{blue}{ E[X_{j}(t)X_{k}(t)] - m_{j}(t)m_{k}(t)}                                                                                                                                                                                                                                                                               \\
                       & = \textcolor{blue}{  -\partial z_{j} \partial z_{k} \widehat{\mu_{t}}(z)|_{z=0} - m_{j}(t)m_{k}(t)}                                                                                                                                                                                                                                       \\
                       & = \textcolor{blue}{  -\partial z_{j} \left( \partial z_{k} (\ln \widehat{\mu_{t}}(z)) \widehat{\mu_{t}}(z) \right) |_{z=0} - m_{j}(t)m_{k}(t)}                                                                                                                                                                                            \\
                       & = \textcolor{blue}{  -\partial z_{j} \partial z_{k}|_{z=0} (\ln \widehat{\mu_{t}}(z)) \widehat{\mu_{t}}(z)   -\partial z_{j}( \ln\widehat{\mu_{t}}(z) )\partial z_{k}( \ln\widehat{\mu_{t}}(z) )\widehat{\mu_{t}}(z)|_{z=0} - m_{j}(t)m_{k}(t)}                                                                                           \\
                       & = \textcolor{blue}{  -\partial z_{j} \partial z_{k}|_{z=0} (\ln \widehat{\mu_{t}}(z)) \cancelto{1}{\widehat{\mu_{t}}(z)}   -\cancelto{i m_{j}(t)}{\partial z_{j}( \ln\widehat{\mu_{t}}(z) )}\cancelto{im_{k}(t)}{\partial z_{k}( \ln\widehat{\mu_{t}}(z) )} \hspace{0.3cm}\cancelto{1}{\widehat{\mu_{t}}(z)}\,|_{z=0} - m_{j}(t)m_{k}(t)} \\
                       & = \textcolor{blue}{  -\partial z_{j} \partial z_{k}|_{z=0} (\ln \widehat{\mu_{t}}(z)) }
        \end{align*}
        We didn't check that integration and differentiation can be exchanged.
    }

the $(j, k)$ elements of the covariance matrix of $X(t)$ \textcolor{blue}{calculated with the cumulant generating function}, are expressed as


\begin{equation*}
    v_{j k}(t)=t\left(A_{j k}+\int_{\mathbb{R}^{d}} x_{j} x_{k} \nu(\mathrm{d} x)\right) \tag{25.8}
\end{equation*}

\textcolor{blue}{
    \begin{align*}
        -\partial z_{j} \partial z_{k} \left( -\frac{t}{2} \left< z,Az \right> \right) & =
        \frac{t}{2}\partial z_{j} \left( \left<e_{k}, Az \right> + \left<z , Ae_{k} \right>  \right)                                                                                                                                            \\
                                                                                       & = \frac{t}{2} \left( \left<e_{k}, Ae_{j} \right> + \left<e_{j} , Ae_{k} \right>  \right)                                                               \\
                                                                                       & = tA_{jk}                                                                                                                                              \\
        -\partial z_{j} \partial z_{k} \left(  i\left< \gamma,z \right> \right)        & = -i \partial z_{j} \left< \gamma, e_{k} \right>                                                                                                       \\
                                                                                       & = 0                                                                                                                                                    \\
        -\partial z_{j} \partial z_{k}|_{z=0}                                          & \left(t \int_{\mathbb{R}^{d}} \bigl( \operatorname{e x p} ( i \langle z, x \rangle)-1-i \langle z, x \rangle I_{D} ( x ) \bigr) \, \nu( d x )  \right) \\
                                                                                       & = -t\int_{\mathbb{R}^{d}}  \partial z_{j} \partial z_{k} |_{z=0}\operatorname{e x p} ( i \langle z, x \rangle) \nu( d x )                              \\
                                                                                       & = t\int_{\mathbb{R}^{d}} x_{j} x_{k} \nu( d x )
    \end{align*}
}


Theorem 25.3 shows that, for a Lévy process $\left\{X_{t}\right\}$ with Lévy measure $\nu$,
the tails of $P_{X_{t}}$ and $\nu$ have a kind of similarity. Are they actually equivalent
in some class of Lévy processes? This question was answered by Embrecht, Goldie, and Veraverbeke [109]
for subordinators. We state their result without proof in two remarks below. \\


THEOREM 25.17 (Exponential moment). Let $\left\{X_{t}\right\}$ be a Lévy process on $\mathbb{R}^{d}$
generated by $(A, \nu, \gamma)$. Let

$$
    C=\left\{c \in \mathbb{R}^{d}: \int_{|x|>1} \mathrm{e}^{\langle c, x\rangle} \nu(\mathrm{d} x)<\infty\right\}
$$

(i) The set $C$ is convex and contains the origin. \\

(ii) $c \in C$ if and only if $E \mathrm{e}^{\left\langle c, X_{t}\right\rangle}<\infty$ for some $t>0$ or,
equivalently, for every $t>0$.\\

(iii) If $w \in \mathbb{C}^{d}$ is such that $\operatorname{Re} w \in C$, then


\begin{equation*}
    \Psi(w)=\frac{1}{2}\langle w, A w\rangle+\int_{\mathbb{R}^{d}}\left(\mathrm{e}^{\langle w, x\rangle}-1-\langle w, x\rangle 1_{D}(x)\right) \nu(\mathrm{d} x)+\langle\gamma, w\rangle \tag{25.11}
\end{equation*}


is definable, $E\left|\mathrm{e}^{\left(w, X_{t}\right)}\right|<\infty$, and


\begin{equation*}
    E\left[\mathrm{e}^{\left\langle\boldsymbol{w}, X_{t}\right\rangle}\right]=\mathrm{e}^{t \Psi(w)} \tag{25.12}
\end{equation*}

\begin{proof}
    (i) Obviously $C$ contains the origin. If $c_{1}$ and $c_{2}$ are in $C$, then, for any $0<r<1$ and $s=1-r$,

    $$
        \int_{|x|>1} e^{\left\langle r c_{1}+s c_{2}, x\right\rangle} \nu(\mathrm{d} x) \leq\left(\int_{|x|>1} e^{\left\langle c_{1}, x\right\rangle } \nu(\mathrm{d} x)\right)^{\tau}\left(\int_{|x|>1} e^{\left\langle c_{2}, x\right\rangle} \nu(\mathrm{d} x)\right)^{s}<\infty
    $$

    by Hölder's inequality. Hence $C$ is convex.\\

    (ii) The function $g(x)=\mathrm{e}^{(c, x)}$ is clearly submultiplicative. Hence Theorem 25.3 gives the assertion. \\

    (iii). Any linear transformation $U$ of $\mathbb{R}^{d}$ to $\mathbb{R}^{d}$ can be uniquely extended to a linear transformation of $\mathbb{C}^{d}$ to $\mathbb{C}^{d}$. Regarding $U$ as a $d \times d$ matrix, it is easy to see that $\langle w, U v\rangle=\left\langle U^{\prime} w, v\right\rangle$ for $w, v \in \mathbb{C}^{d}$, where $U^{\prime}$ is the transpose of $U$. Now let $\operatorname{Re} w \in C$. Then $\int_{|x|>1}\left|\mathrm{e}^{\langle\boldsymbol{w}, x\rangle}\right| \nu(\mathrm{d} x)=\int_{|x|>1} \mathrm{e}^{\langle\operatorname{Re} w, x\rangle} \nu(\mathrm{d} x)<\infty$, which shows that $\Psi(w)$ of (25.11) is definable and finite. Also, $E\left|\mathrm{e}^{\left\langle w, X_{t}\right\rangle}\right|=$ $E e^{\left(R e v, X_{t}\right\rangle}<\infty$ by (ii). Let us show (25.12) in three steps.

    Step 1. Let $e_{1}$ be the unit vector with first component 1. Assume that $e_{1} \in C$. Let us prove (25.12) for all $w=\left(w_{j}\right)_{1 \leq j \leq d}$ with $\operatorname{Re} w_{1} \in[0,1]$ and\\
    $\operatorname{Re} w_{j}=0,2 \leq j \leq d$. Fix $t>0$ and $w_{2}, \ldots, w_{d} \in \mathbb{C}$ with $\operatorname{Re} w_{j}=0$, $2 \leq j \leq d$, and regard $w_{1}$ as variable in $F=\left\{w_{1} \in \mathbb{C}: \operatorname{Re} w_{1} \in[0,1]\right\}$. Consider $f\left(w_{1}\right)=E \mathrm{e}^{\left\langle\boldsymbol{w}, X_{t}\right\rangle}$.
    Then $f\left(w_{1}\right)$ is continuous on $F$, since

    $$
        \left|e^{\langle\boldsymbol{w}, X(t)\rangle}\right|=e^{\left(\operatorname{Re} w_{1}\right) X_{1}(t)} \leq\left(\operatorname{Re} w_{1}\right) e^{X_{1}(t)}+\left(1-\operatorname{Re} w_{1}\right) \leq e^{X_{1}(t)}+1
    $$

    by the convexity of $\mathrm{e}^{u X_{1}(t)}$ in $u$, where $X_{1}(t)$ is the first component of $X(t)$.
    Moreover, $f\left(w_{1}\right)$ is analytic in the interior of $F$, since it is the limit of the analytic
    functions $E\left[e^{\left\langle\boldsymbol{v}, X_{t}\right\rangle} ;\left|X_{t}\right| \leq n\right]$ as $n \rightarrow \infty$.
    Similarly, $h\left(w_{1}\right)=$ $\mathrm{e}^{\mathrm{tw}(w)}$ is continuous on $F$ and analytic in the interior of $F$.
    If $\operatorname{Re} w_{1}=0$, then $f\left(w_{1}\right)=h\left(w_{1}\right)$, which is the Lévy-Khintchine representation
    of $P_{X_{i}}$. Therefore, as in the proof of Theorem 24.11, the principle of reflection and the uniqueness theorem yield (25.12)
    when $\operatorname{Re} w_{1} \in[0,1]$.

    Step 2. Let $U$ be a linear transformation from $\mathbb{R}^{d}$ onto $\mathbb{R}^{d}$. Let $Y_{t}=$ $U X_{t}$.
    Then $\left\{Y_{t}\right\}$ is a Lévy process with generating triplet $\left(A_{U}, \nu_{U}, \gamma_{U}\right)$
    by Proposition 11.10. Write

    $$
        C_{U}=\left\{c \in \mathbb{R}^{d}: \int_{|x|>1} e^{\langle c, x\rangle} \nu_{U}(\mathrm{~d} x)<\infty\right\}
    $$

    Since $\nu_{U}=\nu U^{-1}$, we have $C_{U}=\left(U^{\prime}\right)^{-1} C$. Given $w \in \mathbb{C}^{d}$
    satisfying Re $w \in$ $C$, let $v=\left(U^{\prime}\right)^{-1} w$. Then $\operatorname{Re} v \in C_{U}$. Define

    $$
        \Psi_{U}(v)=\frac{1}{2}\left\langle v, A_{U} v\right\rangle+\int_{\mathbb{R}^{d}}\left(\mathrm{e}^{\langle v, x\rangle}-1-\langle v, x\rangle 1_{D}(x)\right) \nu_{U}(\mathrm{~d} x)+\left\langle\gamma_{U}, v\right\rangle
    $$

    We claim that if

    \begin{equation*}
        E\left[e^{\left\langle v_{i} Y_{t}\right\rangle}\right]=\mathrm{e}^{t \Psi_{U}(v)} \tag{25.13}
    \end{equation*}

    then $w$ satisfies (25.12). In fact, $\left\langle v, Y_{t}\right\rangle=\left\langle\left\langle U^{-1}\right\rangle^{\prime} w, Y_{t}\right\rangle=\left\langle w, X_{t}\right\rangle$
    and

    $$
        \begin{aligned}
            \Psi_{U}(v) & =\frac{1}{2}\left\langle U^{\prime} v, A U^{\prime} v\right\rangle+\int\left(\mathrm{e}^{\langle v, U x\rangle}-1-\langle v, U x\rangle 1_{D}(x)\right) v(\mathrm{~d} x)+\langle U \gamma, v\rangle \\
                        & =\Psi(w)
        \end{aligned}
    $$

    by (11.8)-(11.10). That is, (25.13) is identical with (25.12). \\

    Step 3. Given $w \in \mathbb{C}^{d}$ satisfying $\operatorname{Re} w \in C$, we shall
    show (25.12). If Re $w=0$, there is nothing to prove. Assume Re $w \neq 0$. Choose a
    linear transformation $U$ from $\mathbb{R}^{d}$ onto $\mathbb{R}^{d}$ such that $\operatorname{Re} w=U^{\prime} e_{1}$.
    Consider the Lévy process $Y_{t}=U X_{t}$. Since $C_{U}=\left(U^{\prime}\right)^{-1} C$, we have $e_{1} \in C_{U}$.
    We know, by Step 1, that, if $v \in \mathbb{C}^{d}$ satisfies $\operatorname{Re} v=e_{1}$, then (25.13) holds.
    Hence, by the result of Step 2, $w$ satisfies (25.12).

\end{proof}


We close this section with a discussion of the $g$-moments of $\sup _{s \in[0, t]}\left|X_{s}\right|$ \\


THEOREM 25.18. Let $\left\{X_{t}\right\}$ be a Lévy process on $\mathbb{R}^{d}$. Define

\begin{equation*}
    X_{t}^{*}=\sup _{s \in[0, t]}\left|X_{s}\right| \tag{25.14}
\end{equation*}

Let $g(r)$ be a nonnegative continuous submultiplicative function on $[0, \infty)$, increasing to $\infty$ as $r \rightarrow \infty$.
Then the following four statements are equivalent.

(1) $E\left[g\left(X_{t}^{*}\right)\right]<\infty$ for some $t>0$. \\

(2) $E\left[g\left(X_{t}^{*}\right)\right]<\infty$ for every $t>0$. \\

(3) $E\left[g\left(\left|X_{t}\right|\right)\right]<\infty$ for some $t>0$. \\

(4) $E\left[g\left(\left|X_{t}\right|\right)\right]<\infty$ for every $t>0$. \\

\begin{proof}
    Since $g(|x|)$ is submultiplicative on $\mathbb{R}^{d}$, (3) and (4) are equivalent by
    Theorem 25.3. As $\left|X_{t}\right| \leq X_{t}^{*}$ \textcolor{blue}{$ \implies g(\left|X_{t}\right|) \leq g(X_{t}^{*})$}, all we have to show is that, for
    any fixed $t>0$, $E\left[g\left(\left|X_{t}\right|\right)\right]<\infty$ implies
    $E\left[g\left(X_{t}^{*}\right)\right]<\infty$. We claim that, for any $a>0$ and $b>0$,

    \begin{equation*}
        P\left[X_{t}^{*}>a+b\right] \leq P\left[\left|X_{t}\right|>a\right] / P\left[X_{t}^{*} \leq b / 2\right] \tag{25.15}
    \end{equation*}

    Fix $t$ and let $t_{n, j}=j t / 2^{n}$ for $j=1, \ldots, 2^{n}$ and $X_{(n)}^{*}=\max _{1 \leq j \leq 2^{n}}\left|X_{t_{n, j}}\right|$. Choosing $Z_{j}(s)=Z_{j}=X_{t_{n, j}}-X_{t_{n, j-1}}$ in Lemma 20.2 and using Remark 20.3, we have

    $$
        P\left[X_{(n)}^{*}>a+b\right] \leq P\left[\left|X_{t}\right|>a\right] / P\left[X_{(n)}^{*} \leq b / 2\right]
    $$

    in (20.2). Hence, letting $n \rightarrow \infty$, we get (25.15).
    Choose $b>0$ such that $P\left[X_{t}^{*} \leq b / 2\right]>0$.
    Let $\tilde{g}(r)$ be a continuous increasing function on $[0, \infty)$
                    such that $\bar{g}(0)=0$ and $\tilde{g}(r)=g(r)$ for $r \geq 1$.
                    Apply Lemma 17.6 to $\left.k(r)=1-P|| X_{t} \mid \leq r\right]$ and $l(r)=\tilde{g}(r)$. Then

    $$
        \int_{0+}^{\infty} P\left[\left|X_{t}\right|>r\right] \mathrm{d} \tilde{g}(r)=\int_{(0, \infty)} \tilde{g}(r) P\left[\left|X_{t}\right| \in \mathrm{d} r\right]=E\left[\widetilde{g}\left(\left|X_{t}\right|\right)\right]
    $$

    14 follows from (25.15) that

    $$
        \int_{0+}^{\infty} P\left[X_{t}^{*}>r+b\right] \mathrm{d} \widetilde{g}(r) \leq E\left[\widetilde{g}\left(\left|X_{t}\right|\right)\right] / P\left[X_{t}^{*} \leq b / 2\right]
    $$

    The integral in the left-hand side equals

    $$
        \int_{(0, \infty)} \tilde{g}(r) P\left[X_{t}^{*}-b \in \mathrm{d} r\right]=E\left[\tilde{g}\left(X_{t}^{*}-b\right) ; X_{t}^{*}>b\right]
    $$

    similarly. Hence, if $E\left[g\left(\left|X_{t}\right|\right)\right]<\infty$,
    then $E\left[g\left(X_{t}^{*}-b\right) ; X_{t}^{*}>b\right]<\infty$ and,
    by the submultiplicativity of $g, E\left[g\left(X_{t}^{*}\right)\right]<\infty$. \\

\end{proof}

\printbibliography

\end{document}