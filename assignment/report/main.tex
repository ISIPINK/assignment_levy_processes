\documentclass[a4paper,11pt]{article}

\setlength{\textwidth}{15.0cm}
\setlength{\textheight}{24.0cm}
\setlength{\topmargin}{0cm}
\setlength{\headsep}{0cm}
\setlength{\headheight}{0cm}
\setlength{\parindent}{0pt}
\pagestyle{plain}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, mathtools, amsthm, amssymb}
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cancel}

\allowdisplaybreaks
\input{environments.tex}

\usepackage[style=authoryear,backend=biber]{biblatex}

\addbibresource{bibliography.bib} 

\title{Assignment Levy processes}

\author{}
\date{}


\begin{document}

\maketitle

\begin{abstract}
    We elaborate some results from chapter $25$ of \citeauthor{sato_levy_2013}'s \citetitle{sato_levy_2013} \cite{sato_levy_2013}. Chapter $25$ of \cite{sato_levy_2013} discusses properties of $g$-moments of Lévy processes and their relation with the Lévy measure.
\end{abstract}

\section{Proof of Existence of Moments from Smoothness of Characteristic Function}

In this section we prove the partial reverse of the existence of moments implies smoothness of the characteristic function.

\begin{definition}[$\Delta^{k}_{h}$]
    We define $\forall h>0: \Delta_{h}$ ($h$-central difference) of $f:\mathbb{R}\rightarrow \mathbb{R}$ as follows:
    \begin{equation}
        \Delta_{h} f(t) = f(t+h)- f(t-h),
    \end{equation}
    and $\forall k \in \mathbb{N}:\Delta^{k}_{h}$ is defined as applying $\Delta_{h}$ applying $k$ times and
    $k=0$ as the identity. Note that $\Delta^{k}_{h}$ is a linear operator. We will always take
    differences in respect to the $t$ variable.
\end{definition}

\begin{lemma}[$\Delta^{k}_{h} e^{at}$]
    \begin{equation}
        \forall k \in \mathbb{N}: \Delta^{k}_{h} e^{at} =  e^{at} \left( e^{ah} - e^{-ah} \right)^{k}.
    \end{equation}
\end{lemma}

\begin{proof}
    This follows simply by induction. $k=0$ is trivial.
    Base case:
    \begin{align}
        \Delta_{h} e^{at} & =\left( e^{a(t+h)} - e^{a(t-h)} \right) \\
                          & =e^{at}\left( e^{ah} - e^{-ah} \right)
        .
    \end{align}
    Induction hypothesis:
    \begin{equation}
        \Delta^{k-1}_{h} e^{at} = e^{at} \left( e^{ah} - e^{-ah} \right)^{k-1}.
    \end{equation}

    Induction step:
    \begin{align}
        \Delta^{k}_{h} e^{at} & = \Delta_{h} \Delta^{k-1}_{h} (e^{at})                                        \\
                              & = \Delta_{h} \left( e^{at} \left( e^{ah} - e^{-ah} \right)^{k-1} \right)      \\
                              & = \Delta_{h} \left( e^{at}\right) \left( e^{ah} - e^{-ah} \right)^{k-1}       \\
                              & = e^{at}\left( e^{ah} - e^{-ah} \right) \left( e^{ah} - e^{-ah} \right)^{k-1} \\
                              & =  e^{at} \left( e^{ah} - e^{-ah} \right)^{k}.
    \end{align}
\end{proof}


\begin{theorem}[central finite difference formula]
    $\forall k \in \mathbb{N}$ $\forall$  $k$-continuously differentiable $f$'s at $0$:
    \begin{equation}
        f^{(k)}(0) = \lim_{h \to 0} \frac{\Delta^{k}_{h} f(t)|_{t=0}}{(2h)^{k}}.
    \end{equation}
\end{theorem}

\begin{proof}
    Follows by the mean value theorem.
    \begin{align}
         & \lim_{h \to 0} \frac{\Delta^{k}_{h} f(t)|_{t=0}}{(2h)^{k}}                                                                              \\
         & = \lim_{h \to 0} \frac{\Delta_{h}\Delta^{k-1}_{h} f(t)|_{t=0}}{(2h)^{k}}                                                                \\
         & = \lim_{h \to 0} \frac{\Delta^{k-1}_{h} f(t)(|_{t=h}-|_{t=-h})}{(2h)^{k}}                                                               \\
         & = \lim_{h \to 0} \frac{2h (\Delta^{k-1}_{h} f(t))'|_{t=z^{1}_{h}}}{(2h)^{k}}, z^{1}_{h} \in[-h,h]                                       \\
         & = \lim_{h \to 0} \frac{\Delta^{k-1}_{h} f'(t)|_{t=z^{1}_{h}}}{(2h)^{k-1}}, z^{1}_{h} \in[-h,h]                                          \\
         & = \lim_{h \to 0} \frac{\Delta^{k-2}_{h} f^{(2)}(t)|_{t=z^{2}_{h}}}{(2h)^{k-2}}, z^{2}_{h} \in[z^{1}_{h}-h,z^{1}_{h}+h] \subset [-2h,2h] \\
         & \quad ...                                                                                                                               \\
         & = \lim_{h \to 0} f^{(k)}(t)|_{t=z^{k}_{h}},  z^{k}_{h} \in [-k h, k h]                                                                  \\
         & = \lim_{h \to 0} f^{(k)}(z^{k}_{h}) = f^{(k)}(0).
    \end{align}
    Last line follows because $f^{k}$ is continuous at $0$. This proof may need some modification because we need to assume that there exist a neighborhood around $0$ where $f$ is $k$-times continuously differentiable instead only $k$-times continuously differentiable at $0$.
\end{proof}


\begin{lemma} \label{lemma:moment_smooth_char}
    $\forall k \in  \mathbb{N}:\phi_{X}(t)$ is $2k$-time continuous differentiable $\Rightarrow  E[X^{2k}] < \infty$
\end{lemma}

\begin{proof}
    Use the central finite difference formula and previous lemma:
    \begin{align}
        |\phi_{X}^{(2k)}(0)| & = \left|\lim_{h \to 0} \frac{\Delta^{2k}_{h}|_{t=0} \phi_{X}(t)}{(2h)^{2k}}\right|                      \\
                             & = \left|\lim_{h \to 0} \frac{\Delta^{2k}_{h}|_{t=0} E\left[e^{itX}\right]}{(2h)^{2k}}\right|            \\
                             & = \left|\lim_{h \to 0} \frac{ E\left[\Delta^{2k}_{h}|_{t=0}e^{itX}\right]}{(2h)^{2k}}\right|            \\
                             & = \left|\lim_{h \to 0} \frac{ E\left[e^{itX} |_{t=0} (e^{ihX}-e^{-ihX})^{2k}\right]}{(2h)^{2k}}\right|  \\
                             & = \left|\lim_{h \to 0} \frac{ E\left[(e^{ihX}-e^{-ihX})^{2k}\right]}{(2h)^{2k}}\right|                  \\
                             & = \left|\lim_{h \to 0} \frac{1}{i^{2k}}E\left[\frac{ (e^{ihX}-e^{-ihX})^{2k}}{(2h)^{2k}} \right]\right| \\
                             & = \lim_{h \to 0} E\left[\frac{ \sin(Xh)^{2k}}{h^{2k}}\right]                                            \\
                             & = \lim_{h \to 0} E\left[ X^{2k}\frac{ \sin(Xh)^{2k}}{X^{2k} h^{2k}}\right]                              \\
                             & = \lim_{h \to 0} E\left[ X^{2k} \operatorname{sinc}(Xh) ^{2k}\right]                                    \\
                             & \ge  E\left[ X^{2k} \right]
    \end{align}
    Where last line follows by Fatou's lemma.
\end{proof}

\section{Submultiplicative Functions}
In this section we introduce submultiplicative functions and prove some properties of them.

\begin{definition}[submultiplicative function]
    A function $g(x)$ on $\mathbb{R}^{d}$ is submultiplicative if and only if it is nonnegative
    and $\exists a \in \mathbb{R}>0:$

    \begin{equation}
        g(x+y) \leq a g(x) g(y) \text { for } x, y \in \mathbb{R}^{d}
        .
    \end{equation}

\end{definition}

We will use following lemma implicitly in the following proofs.

\begin{lemma}[induction on submultiplicativity]
    Let $g(x)$ be a submultiplicative with constant $a$ then:
    \begin{equation}
        \forall (x_{i})_{i} \subset \mathbb{R}^d:
        g\left(\sum_{i=1}^{n} x_{i}\right) \le a^{n-1} \prod_{i=1}^{n} g(x_{i})  .
    \end{equation}
\end{lemma}

\begin{proof}
    This follows by induction on $n$. \\
    Base case: $n=2$ is true by submultiplicativity. \\
    Induction hypothesis: assume $2 \le  n \le k$. \\
    Induction step: $n=k+1$:
    \begin{align}
        \forall (x_{i})_{i}                  & \subset \mathbb{R}^d:                                                                          \\
        g\left(\sum_{i=1}^{k+1} x_{i}\right) & = g\left(\sum_{i=1}^{k} x_{i} + x_{k+1}\right)                                                 \\
                                             & \le a g\left(\sum_{i=1}^{k} x_{i}\right) g(x_{k+1})  \text{ by submultiplicativity}            \\
                                             & \le a a^{k-1} \prod_{i=1}^{k}  g\left(x_{i}\right) g(x_{k+1})  \text{ by induction hypothesis} \\
                                             & \le a^{k} \prod_{i=1}^{k+1}  g\left(x_{i}\right) .
    \end{align}
\end{proof}

\begin{lemma}
    The product of two submultiplicative functions is submultiplicative.
\end{lemma}

\begin{proof}
    Let $g,h:\mathbb{R}^{d}\rightarrow \mathbb{R}$ be submultiplicative functions then:
    $$
        \forall x,y \in \mathbb{R}^{d}: g(x+y)h(x+y) \le a_{1} g(x)g(y) a_{2} h(x)h(y) \leq a_{1}a_{2} g(x)h(x)g(y)h(y).
    $$
    So $gh:\mathbb{R}^{d}\rightarrow \mathbb{R}$ is submultiplicative.
\end{proof}

\begin{lemma}
    If $g(x)$ is submultiplicative on $\mathbb{R}^{d}$, then so is $g(c x+\gamma)^{\alpha}$
    with $c \in \mathbb{R}$, $\gamma \in \mathbb{R}^{d}$, and $\alpha>0$.
\end{lemma}

\begin{proof}
    Let $g_{1}(x)=g(c x), g_{2}(x)=g(x+\gamma)$, and $g_{3}(x)=g(x)^{\alpha}$.
    \begin{align}
        \forall x,y & \in \mathbb{R}^{d}:                                                                                                                 \\
        g_{1}(x+y)  & = g(c(x+y)) = g(c x + c y) \leq a g(c x) g(c y) = a g_{1}(x) g_{1}(y)                                                               \\
        g_{2}(x+y)  & = g(x + y + \gamma) = g(x + \gamma + y + \gamma - \gamma)
        \le a^{2} g(-\gamma) g(x+\gamma) g(y + \gamma)                                                                                                    \\
                    & \le a^{2} g(-\gamma) g_{2}( x)g_{2}(y)                                                                                              \\
        g_{3}(x+y)  & = g(x+y) ^{\alpha} \le \left(a g(x) g(y) \right) ^{\alpha}= a^{\alpha} g(x)^{\alpha} g(y)^{\alpha} = a^{\alpha} g_{3}(x) g_{3}(y) .
    \end{align}
\end{proof}

\begin{example}[submultiplicative functions]\label{ex:submult_func}
    The class of submultiplicative functions is broad. Let $0<\beta \leq 1$. Then the following functions are submultiplicative:
    \begin{gather}
        |x| \vee 1, \left|x_{j}\right| \vee 1, x_{j} \vee 1, \exp \left(|x|^{\beta}\right), \exp \left(\left|x_{j}\right|^{\beta}\right) \\
        \exp \left(\left(x_{j} \vee 0\right)^{\beta}\right), \log (|x| \vee e), \log \left(\left|x_{j}\right| \vee e\right), \log \left(x_{j} \vee \mathrm{e}\right) \\
        \log \log \left(|x| \vee e^{e}\right), \log \log \left(\left|x_{j}\right| \vee e^{e}\right), \log \log \left(x_{j} \vee e^{e}\right)
    \end{gather}
\end{example}

\begin{proof}
    See \cite{sato_levy_2013} page 159 PROPOSITION 25.4 (iii).
\end{proof}

\section{Finiteness of the $g$-moments of a L\'{e}vy process}

In this section we characterize the finiteness of the $g$-moment of a Lévy process in terms of the Lévy measure.

\begin{definition}[$g$-moment]
    Let $g(x)$ be a nonnegative measurable function on $\mathbb{R}^{d}$. We call
    $\int g(x) \mu(\mathrm{d} x)$ the $g$-moment of a measure $\mu$ on $\mathbb{R}^{d}$. We call $E[g(X)]$ the $g$-moment of a random variable $X$ on $\mathbb{R}^{d}$.
\end{definition}

\begin{definition}[locally bounded function]
    A function is locally bounded if and only if it is bounded on every compact set.
\end{definition}

The main result of the assignment is following theorem.


\begin{theorem}[charaterization of finiteness of the $g$-moment of a L\'{e}vy process] \label{th:char finite moment}
    Let $g$ be a submultiplicative, locally bounded, measurable
    function on $\mathbb{R}^{d}$ and let $\left\{X_{t}\right\}$ be a Lévy process
    on $\mathbb{R}^{d}$ with Lévy measure $\nu$. Then, $X_{t}$ has finite $g$-moment for every $t>0$ if
    and only if $[\nu]_{\{|x|>1\}}$ has finite $g$-moment.
\end{theorem}

\begin{corollary}[time independent property]
    Let $g$ be a submultiplicative, locally bounded, measurable
    function on $\mathbb{R}^{d}$. Then, finiteness of the $g$-moment is not a time dependent
    distributional property in the class of Lévy processes.
\end{corollary}
\begin{proof}
    Follows directly from Theorem \ref{th:char finite moment}.
\end{proof}

We prove Theorem \ref{th:char finite moment} after three lemmas.

\begin{lemma}[exponential bound for submultiplicative functions] \label{lem:exp bound}
    If $g(x)$ is submultiplicative and locally bounded, then

    \begin{equation}
        g(x) \leq b e^{c|x|}
    \end{equation}
    with some $b>0$ and $c>0$.
\end{lemma}

\begin{proof}
    $\sup _{|x| \leq 1} g(x) < \infty$ because $g$ is locally bounded, let $b = \max(\sup _{|x| \leq 1} g(x),\frac{1}{a}+1)$ such that $\sup _{|x| \leq 1} g(x)\le b$ and $ab>1$.
    $\forall x$ set $n_x = \lceil \left| x \right| \rceil$ implying $n_x-1 < \left| x \right| \leq n_x, |\frac{x}{n_x}|\le 1$ and $g(\frac{x}{n _{x}})\le b$, then
    \begin{equation}
        g(x) = g\left(\sum_{i=1}^{n_x} \frac{x}{n_x}\right)
        \leq a^{n_x-1} g\left(\frac{x}{n_x} \right)^{n_x} \leq a^{n_x-1} b^{n_x} \leq b(a b)^{|x|}
        \le b e^{\ln \left(ab\right)|x|}.
    \end{equation}
\end{proof}

\begin{lemma}\label{lem:supp entire}
    Let $\mu$ be an infinitely divisible distribution on $\mathbb{R} $ with Lévy measure $\nu$
    supported on a bounded set $S\subset[-a, a]$ for some $a>0 \in \mathbb{R}$.
    Then $\widehat{\mu}(t)$ can be extended to an entire function on $\mathbb{C}$.
\end{lemma}

\begin{proof}
    The Lévy representation of $\widehat{\mu}(t)$ is
    \begin{align}
        \widehat{\mu}(t) & =\exp \left[-\frac{1}{2}At^{2} +\int_{[-a, a]}\left(\mathrm{e}^{i tx}-1-itx\right) \nu(dx)+i \gamma't \right]                  \\
                         & =\exp \left[-\frac{1}{2}At^{2}+i \gamma't \right] \exp \left[\int_{-a}^{a}\left(\mathrm{e}^{i tx}-1-itx\right) \nu(dx) \right]
    \end{align}

    with some $\gamma^{\prime} \in \mathbb{R}$.

    As products, sums, compositions and integrals of entire functions are entire (integrals follow from Morera's theorem), $\exp$ and polynomials are entire it is sufficient to show that $\int_{-a}^{a}\left(\mathrm{e}^{i tx}-1-itx\right) \nu(dx)$ is finite for $t \in \mathbb{C}$:

    \begin{align}
         & \left|\int_{-a}^{a}\left(\mathrm{e}^{i tx}-1-itx\right) \nu(dx) \right|                                                                                                 \\
         & \leq \int_{-a}^{a}|\mathrm{e}^{i tx}-1-itx| \nu(dx)                                                                                                                     \\
         & \leq \int_{-1}^{1}|\mathrm{e}^{i tx}-1-itx| \nu(dx) + \int_{[-a,a] \setminus [-1,1]}|\mathrm{e}^{i tx}-1-itx| \nu(dx)                                                   \\
         & \leq \int_{-1}^{1}|\mathrm{e}^{i tx}-1-itx| \nu(dx) + \sup_{x \in [-a,a] \setminus [-1,1]}\left(|\mathrm{e}^{i tx}-1-itx|\right) \int_{[-a,a] \setminus [-1,1]} \nu(dx) \\
         & \leq \int_{-1}^{1}|\mathrm{e}^{i tx}-1-itx| \nu(dx) + I_{2}                                                                                                             \\
         & \leq \int_{-1}^{1}\left|\sum_{n=2}^{\infty} \frac{(ixt)^{n}}{n!}\right| \nu(dx) + I_{2}    \label{line: expansion in rep}                                               \\
         & \leq \int_{-1}^{1}\sum_{n=2}^{\infty} \frac{|x|^{n}|t|^{n}  }{n!} \nu(dx) + I_{2}                                                                                       \\
         & \leq \int_{-1}^{1}|x|^{2} \sum_{n=2}^{\infty} \frac{|x|^{n-2} |t| ^{n}}{n!} \nu(dx) + I_{2}                                                                             \\
         & \leq \int_{-1}^{1}|x|^{2} \sum_{n=2}^{\infty} \frac{|t|^{n}}{n!} \nu(dx) + I_{2}                                                                                        \\
         & \leq  \sum_{n=2}^{\infty} \frac{|t|^{n}}{n!} \int_{-1}^{1}|x|^{2} \nu(dx) + I_{2}                                                                                       \\
         & \leq  (e^{|t| }-|t| -1)  \int_{-1}^{1}|x|^{2} \nu(dx) + I_{2}       \label{line: end expansion in rep}                                                                  \\
         & < \infty
        .
    \end{align}
    We implicitly used $|x|<1 \implies |x|^{n-2}<1$,$|\mathrm{e}^{i tx}-1-itx|$ continuous in $x$ so achieves maximum on compact sets (Weierstrass theorem) and

    \begin{equation}
        \int_{\mathbb{R}} (|x|^{2} \wedge 1) \nu(dx)<\infty \implies \int_{-1}^{1} |x|^{2} \nu(dx)<\infty, \int_{[-a,a] \setminus [-1,1]} \nu(dx)<\infty   .
    \end{equation}

\end{proof}


\begin{lemma}[entire characteristic functions imply finite exponential moments] \label{lem:entire_char_mom}
    If $\mu$ is a probability measure on $\mathbb{R}$ and $\widehat{\mu}(z)$ is extendible to an entire
    function on $\mathbb{C}$, then $\mu$ has finite exponential moments, that is, it has finite $e^{c|x|}$-moment for every $c>0$.
\end{lemma}

\begin{proof}
    As entire functions are infinitely differentiable it follows from Lemma \ref{lemma:moment_smooth_char} that $\alpha_{2n}=\int x^{2n} \mu(\mathrm{d} x) =\int |x|^{2n} \mu(\mathrm{d} x) $ are finite for any $n \geq 1$ and by H\"{o}lder's inequality the uneven (absolute) moments $(\beta_{2n+1}),\alpha_{2n+1}$ are also finite.
    Since $\frac{\mathrm{d}^{n} \widehat{\mu}}{\mathrm{d} z^{n}}(0)=\mathrm{i}^{n} \alpha_{n}$, we have

    $$
        \widehat{\mu}(z)=\sum_{n=0}^{\infty} \frac{1}{n !} \mathrm{i}^{n} \alpha_{n} z^{n}
    $$

    as $\widehat{\mu}(z)$ is entire the radius of convergence of the right-hand side is infinite therefore $ \forall z \in \mathbb{C}: \left| \frac{z^{n} \alpha_{n}}{n!} \right| = \frac{|z|^{n} }{n!} \alpha_{n} \rightarrow 0$ and again by H\"{o}lder's inequality also  $\forall c \in \mathbb{R}:\frac{2^{n}c^{n}}{n!} \beta_{n}\rightarrow 0 \implies \forall \delta>0 \in \mathbb{R}, \exists n _{c} \in  \mathbb{N},  \forall n>n _{\delta} \in \mathbb{N}:\frac{2^{n}c^{n}}{n!} \beta_{n} <\delta$. Now the exponential moments are easily bounded:

    \begin{align}
        \int \mathrm{e}^{c|x|} \mu(\mathrm{d} x) & =  \int \sum_{n=0}^{\infty} \frac{c^{n} |x|^{n} }{n!} \mu(\mathrm{d} x)      \\
                                                 & =  \sum_{n=0}^{\infty}\frac{c^{n} }{n!} \int |x|^{n} \mu(\mathrm{d} x)       \\
                                                 & \le \sum_{n=0}^{\infty} \frac{1}{n !} \beta_{n} c^{n}                        \\
                                                 &
        \le\sum_{n=0}^{n _{\delta}} \frac{1}{n !} \beta_{n} c^{n} + \sum_{n=n _{\delta}}^{\infty} \frac{1}{n !} \beta_{n} c^{n} \\
                                                 &
        \le\sum_{n=0}^{n _{\delta}} \frac{1}{n !} \beta_{n} c^{n} + \sum_{n=n _{\delta}}^{\infty} \frac{\delta}{2^{n}}          \\
                                                 & < \infty.
    \end{align}
    Exchanging the sum and the integral is justified by Tonelli's theorem as the summand is positive.
\end{proof}

\begin{corollary} \label{cor:finite_moments}
    A L\'{e}vy process with a L\'{e}vy measure supported on a bounded set has finite exponential moments.
\end{corollary}

\begin{proof}
    Follows from Lemma's \ref{lem:entire_char_mom}, \ref{lem:supp entire}.
\end{proof}

\begin{theorem}[compound Poisson process expansion] \label{th:compound_poisson expansion}
    Let $Y_{t}$ be compound Poisson process with Lévy measure $\nu$ then
    \begin{equation}
        P_{Y_{t}}= e^{-t\int \nu(dx) }\sum_{n=0}^{\infty}  \frac{t^{n}}{n !} \nu^{n}.
    \end{equation}
\end{theorem}
\begin{proof}
    \begin{align}
        P[Y_{t} \in B] & = P \left[ \sum_{j=0}^{N_{t}} \varepsilon_{j} \in B \right]                                                     \\
                       & = \sum_{n=0}^{\infty} P \left[ \sum_{j=0}^{n} \varepsilon_{j} \in B \right] P[N_{t}=n]                          \\
                       & = \sum_{n=0}^{\infty} P \left[ \sum_{j=0}^{n} \varepsilon_{j} \in B \right]e^{-t\int \nu(dx) }\frac{t^{n}}{n !}
    \end{align}
\end{proof}



\begin{proof}[Proof of Theorem \ref{th:char finite moment}]

    Let $\nu_{0}=[\nu]_{\{|x| \leq 1\}}$ and $\nu_{1}=[\nu]_{\{|x|>1\}}$. Let
    $\left\{X_{t}^{i}\right\}$ be independent Lévy processes  $\mathbb{R}^{d}$ such
    that $\left\{X_{t}\right\} =_{d} \left\{X_{t}^{0}+ X_{t}^{1}\right\}$ and
    $\left\{X_{t}^{1}\right\}$ is compound Poisson with Lévy measure $\nu_{1}$. Let $\mu_{0}^{t}$ and $\mu_{1}^{t}$ be
    the distributions of $X_{t}^{0}$ and $X_{t}^{1}$, respectively.

    Suppose that $X_{t}$ has a finite $g$-moment for some $t>0$. Then
    \begin{equation}
        \infty > E\left[g\left(X_{t}\right)\right]
        = E\left[g\left(X^{0}_{t} + X^{1}_{t}\right)\right]
        = \iint g(x+y) \mu_{1}^{t}(dy) \mu_{0}^{t}(dx) .
    \end{equation}
    If $\forall x \in \mathbb{R}^{d}:\int g(x+y) \mu_{1}^{t}(dy)=\infty \implies E[g(X_{t})] = \infty$ which is a contradiction. So $\exists x \in \mathbb{R}^{d}:\int g(x+y) \mu_{1}^{t}(dy)<\infty$.
    Because $\mu_{1}^{t}$ is a compound Poisson we have by theorem \ref{th:compound_poisson expansion}:
    \begin{equation}
        \sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g(x+y) \nu_{1}^{n}(dy)<\infty.
    \end{equation}

    Since $g(y) \leq a g(-x) g(x+y) \leq a b e^{c|x|} g(x+y)$ by Lemma \ref{lem:exp bound}, we get

    \begin{align}
        \infty & > \sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g(x+y) \nu_{1}^{n}(dy)                                     \\
               & \ge a^{-1} b^{-1} e^{-c|x|} \sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g(y) \nu_{1}^{n}(dy)             \\
               & \ge a^{-1} b^{-1} e^{-c|x|} \frac{t^{n}}{n !} \int g(y) \nu_{1}^{n}(dy)  \quad \forall n \in \mathbb{N}
    \end{align}
    In particular $n=1$ hence $\int g(y) \nu_{1}(d y)<\infty$.

    Conversely, suppose that $\int g(y) \nu_{1}(dy) = G<\infty$ then we have to prove
    that $E\left[g\left(X_{t}\right)\right]$ $<\infty$ for every $t$.

    \begin{align}
        E\left[g\left(X_{t}\right)\right] & =  E\left[g\left(X_{t}^{0} + X_{t}^{1}\right)\right]                                                                                                \\
                                          & \leq aE\left[ g\left(X_{t}^{0}\right) g\left(X_{t}^{1}\right)\right]       \quad \text{by submultiplicativity}                                      \\
                                          & = aE\left[ g\left(X_{t}^{0}\right)\right]  E\left[ g\left(X_{t}^{1}\right)\right]   \quad \text{by independence}                                    \\
                                          & \le a b E\left[\mathrm{e}^{\mathrm{c}\left|X_{t}^{0}\right|}\right] E\left[g\left(X_{t}^{1}\right)\right] \quad \text{by Lemma \ref{lem:exp bound}}
    \end{align}
    So it is sufficient to show that $E\left[g\left(X_{t}^{1}\right) \right]<\infty$ and
    $E\left[\mathrm{e}^{\mathrm{c}\left|X_{t}^{0}\right|}\right] < \infty$
    \begin{align}
        E[g(X^{1}_{t})] & = \int g(x) \mu _{1} ^{t}(dx)                                                                                                                                   \\
                        & =\sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g(x) \nu_{1}^{n}(dx)                 \quad \text{by theorem \ref{th:compound_poisson expansion}}                    \\
                        & =\sum_{n=0}^{\infty} \frac{t^{n}}{n !} \int g\left(\sum_{j} x_{j}\right) \prod_{j}\nu_{1}\left(dx_{j}\right)                                                    \\
                        & \le \sum_{n=0}^{\infty} \frac{t^{n}}{n !} a^{n-1}\int \prod_{j} g\left(x_{j}\right) \prod_{j} \nu_{1}\left(dx_{j}\right)    \quad \text{by submultiplicativity} \\
                        & =\sum_{n=0}^{\infty} \frac{t^{n}}{n !} a^{n-1} \left(\int g(x) \nu_{1}(dx)\right)^{n}                                                                           \\
                        & =a^{-1}\sum_{n=0}^{\infty} \frac{t^{n}}{n !} a^{n} G^{n}                                                                                                        \\
                        & = a^{-1} e^{aGt} < \infty.
    \end{align}

    Let $X_{j}^{0}(t)$, $1 \leq j \leq d$, be the components of $X_{t}^{0}$. Then

    \begin{align}
        E\left[e^{c\left|X_{t}^{0}\right|}\right] & \leq E\left[\exp \left(c \sum_{j=1}^{d}\left|X_{j}^{0}(t)\right|\right)\right] \leq E\left[\prod_{j=1}^{d}\left(e^{c X_{j}^{0}(t)}+e^{-c X_{j}^{0}(t)}\right)\right] \\
                                                  &
        \le  E \left[ \sum_{\alpha_{j} \in \{-1,1\}} e^{c \sum_{ j=1} ^{d} \alpha_{j} X^{0}_{j}(t)} \right]
        \le  \sum_{\alpha_{j} \in \{-1,1\}} E \left[e^{c \sum_{ j=1} ^{d} \alpha_{j} X^{0}_{j}(t)} \right]
    \end{align}

    Since $X^{0}_{t}$ is a Lévy process with a Lévy measure supported on a bounded set
    therefore $\sum_{j=1}^{d} \alpha_{j} X^{0}_{j}(t)$ is also a Lévy process on $\mathbb{R}$ with a Lévy measure supported on a bounded set by \cite{sato_levy_2013} page 65 PROPOSITION 11.10
    therefore it has finite (exponential) moments by corollary \ref{cor:finite_moments}. I.e. $E\left[e^{c\left|X_{t}^{0}\right|}\right] \le \sum_{\alpha_{j} \in \{-1,1\}} E \left[e^{c \sum_{ j=1} ^{d} \alpha_{j} X^{0}_{j}(t)} \right] <  \infty$.
\end{proof}

\begin{corollary}
    Let $\alpha>0,0<\beta \leq 1$, and $\gamma \geq 0$. None of the properties
    $\int|x|^{\alpha} \mu(\mathrm{d} x)<\infty, \int(0 \vee \log |x|)^{\alpha} \mu(\mathrm{d} x)<\infty$, and
    $\int|x|^{\gamma} e^{\alpha|x|^{\beta}} \mu(\mathrm{d} x)<$ $\infty$ is time dependent in the class
    of Lévy processes. For a Lévy process on $\mathbb{R}^{d}$ with Lévy measure $\nu$, each of the properties
    is expressed by the corresponding property of $[\nu]_{\{|x|>1\}}$.
\end{corollary}

\begin{proof}
    This follows from Theorem \ref{th:char finite moment} and Example \ref{ex:submult_func}.
\end{proof}


\begin{lemma}[formulas for the first $2$ moments of a Lévy process]
    Let $\left\{X_{t}\right\}$ be a Lévy process on $\mathbb{R}^{d}$ generated
    by $(A, \nu, \gamma )$. In components, $X_{t}=\left(X_{j}(t)\right), \gamma=\left(\gamma_{j}\right)$,
    and $A=\left(A_{j k}\right)$.
    Then $\int_{\{|x_{j}| >1\}} |x_{j}| \nu(\mathrm{d} x)<\infty \implies m_{j}(t)=E\left[X_{j}(t)\right]< \infty$ and can be expressed as

    \begin{equation}
        m_{j}(t)=t\left(\int_{|x|>1} x_{j} \nu(\mathrm{d} x)+\gamma_{j}\right).
    \end{equation}

    Similarly $\int_{\{|x_{j}|,|x_{k}|  >1\}} |x_{j}||x_{k}| \nu(\mathrm{d} x),\int_{\{|x_{j}| >1\}} |x_{j}|^{2} \nu(\mathrm{d} x),\int_{\{|x_{k}| >1\}} |x_{k}|^{2} \nu(\mathrm{d} x)<\infty  \implies v_{jk}=E\left[\left(X_{j}(t)-m_{j}(t)\right)\left(X_{k}(t)-m_{k}(t)\right)\right], v_{j j}, v_{k k}< \infty $
    and can be expressed as

    \begin{equation}
        v_{j k}(t)=t\left(A_{j k}+\int_{\mathbb{R}^{d}} x_{j} x_{k} \nu(\mathrm{d} x)\right).
    \end{equation}



\end{lemma}

\begin{proof}
    The finiteness conditions follow from using the fact that projections of Lévy processes are again Lévy processes \cite{sato_levy_2013} page 65 PROPOSITION 11.10 and Theorem \ref{th:char finite moment}. The formulas are derived
    from the derivatives of the characteristic function or cumulant function. I.e.
        { \small
            \begin{align}
                m_{j}(t)   & = -i \partial z_{j}|_{z=0} \widehat{\mu_{t}}(z)                                                                                                                                                                                                                                                                       \\
                           & = -i \partial z_{j}|_{z=0} e^{\ln \left(\widehat{\mu_{t}}(z)\right)}                                                                                                                                                                                                                                                  \\
                           & = -i \partial z_{j}|_{z=0}\ln \left(\widehat{\mu_{t}}(z)\right) \cancelto{1}{\widehat{\mu_{t}}(0)}                                                                                                                                                                                                                    \\
                           & = -i \partial z_{j}|_{z=0}\ln \left(\widehat{\mu_{t}}(z)\right),                                                                                                                                                                                                                                                      \\
                v_{j k}(t) & =E\left[\left(X_{j}(t)-m_{j}(t)\right)\left(X_{k}(t)-m_{k}(t)\right)\right]                                                                                                                                                                                                                                           \\
                           & = E[X_{j}(t)X_{k}(t)] - m_{j}(t)m_{k}(t)                                                                                                                                                                                                                                                                              \\
                           & = -\partial z_{j} \partial z_{k} \widehat{\mu_{t}}(z)|_{z=0} - m_{j}(t)m_{k}(t)                                                                                                                                                                                                                                       \\
                           & = -\partial z_{j} \left( \partial z_{k} (\ln \widehat{\mu_{t}}(z)) \widehat{\mu_{t}}(z) \right) |_{z=0} - m_{j}(t)m_{k}(t)                                                                                                                                                                                            \\
                           & = -\partial z_{j} \partial z_{k}|_{z=0} (\ln \widehat{\mu_{t}}(z)) \widehat{\mu_{t}}(z)   -\partial z_{j}( \ln\widehat{\mu_{t}}(z) )\partial z_{k}( \ln\widehat{\mu_{t}}(z) )\widehat{\mu_{t}}(z)|_{z=0} - m_{j}(t)m_{k}(t)                                                                                           \\
                           & = -\partial z_{j} \partial z_{k}|_{z=0} (\ln \widehat{\mu_{t}}(z)) \cancelto{1}{\widehat{\mu_{t}}(z)}   -\cancelto{i m_{j}(t)}{\partial z_{j}( \ln\widehat{\mu_{t}}(z) )}\cancelto{im_{k}(t)}{\partial z_{k}( \ln\widehat{\mu_{t}}(z) )} \hspace{0.3cm}\cancelto{1}{\widehat{\mu_{t}}(z)}\,|_{z=0} - m_{j}(t)m_{k}(t) \\
                           & = -\partial z_{j} \partial z_{k}|_{z=0} (\ln \widehat{\mu_{t}}(z))
            \end{align}
        }

    The cumulant function of a Lévy process is given by the Lévy-Khintchine formula:
    \begin{equation}
        \frac{\ln \widehat{\mu_{t}}(z)}{t} = -\frac{1} {2} \langle z, A z \rangle +i \langle\gamma, z \rangle+\int_{\mathbb{R}^{d}} \left( \mathrm{e}^{i \langle z, x \rangle}-1-i \langle z, x \rangle I_{D} ( x ) \right) \nu(\mathrm{d} x)
        .
    \end{equation}
    The first $2$ terms are easily differentiated:
    \begin{center}
        \renewcommand{\arraystretch}{1.5} % Increase vertical spacing
        \begin{tabular}{|c|c|c|}
            \hline
            Operator                               & $-\frac{1}{2} \langle z, A z \rangle$ & $i \langle\gamma, z \rangle$ \\ \hline
            $\partial z_{j}|_{z=0}$                & $0$                                   & $\gamma_{j}$                 \\ \hline
            $\partial z_{j} \partial z_{k}|_{z=0}$ & $-A_{jk}$                             & $0$                          \\ \hline
        \end{tabular}
    \end{center}
    Let $I(z)=\int_{\mathbb{R}^{d}} \left( \mathrm{e}^{i \langle z, x \rangle}-1-i \langle z, x \rangle I_{D} ( x ) \right) \nu(\mathrm{d} x)$ then

    \begin{align}
        \partial z_{j}|_{z=0} I(z) & = \lim_{h \to 0} \frac{1}{h} \int_{\mathbb{R}^d} e^{ihx_{j}}-ihx_{j} I_{D}(x)-1 \nu(dx)                                                                                                                     \\
                                   & = \lim_{h \to 0} \frac{1}{h} \int_{D} e^{ihx_{j}}-ihx_{j}-1  \nu(dx) + \frac{1}{h} \int_{D^{c}} e^{ihx_{j}}-1  \nu(dx)                                                                                      \\
                                   & = \cancelto{0}{\lim_{h \to 0} \int_{D} \sum_{n=2}^{\infty} \frac{(ihx_{j})^{n}}{n!h}  \nu(dx)} + \lim_{h \to 0} \frac{1}{h}\int_{D^{c}} ihx_{j}+ e^{ihx_{j}}-ihx_{j}-1  \nu(dx) \label{line:canceled limit} \\
                                   & =  \lim_{h \to 0} \int_{D^{c}}ix_{j} + \frac{e^{ihx_{j}}-ihx_{j}-1}{h} \nu(dx)                                                                                                                              \\
                                   & =  \int_{D^{c}}ix_{j}\nu(dx) + \cancelto{0}{\lim_{h \to 0} \int_{D^{c}}\frac{e^{ihx_{j}}-ihx_{j}-1}{h} \nu(dx)} \label{line:canceled limit2}                                                                \\
                                   & = \int_{D^{c}}ix_{j}\nu(dx)
    \end{align}

    In line (\ref{line:canceled limit}) the canceling follows from $|x_{j}|<|x|$ and doing similar calculations as
    line (\ref{line: expansion in rep}) to line (\ref{line: end expansion in rep}) in proof of Lemma \ref{lem:supp entire} then taking the limit. We still have to elaborate the cancellation on line (\ref{line:canceled limit2}):

    {
    \small
    \begin{align}
         & \left| \lim_{h \to 0} \int_{D^{c}}\frac{e^{ihx_{j}}-ihx_{j}-1}{h} \nu(dx)	 \right|                                                                                                                                                                              \\
         & \le \lim_{h \to 0} \int_{D^{c}} \left| \frac{e^{ihx_{j}}-ihx_{j}-1}{h} \right|  \nu(dx)                                                                                                                                                                         \\
         & = \lim_{h \to 0} \int_{D^{c}} \left| \frac{ix_{j}\int_{0}^{h}e^{isx_{j}}-1ds }{h} \right|  \nu(dx)                                                                                                                                                              \\
         & \le  \lim_{h \to 0} \int_{D^{c}} \frac{h |x_{j}| \sup_{s \in [0,h]}\left|e^{isx_{j}}-1\right| }{h}   \nu(dx)                                                                                                                                                    \\
         & \le  \lim_{h \to 0} \int_{D^{c}}  |x_{j}| \sup_{s \in [0,h]}\left|e^{isx_{j}}-1\right|   \nu(dx)                                                                                                                                                                \\
         & \le  \lim_{h \to 0} \int_{D^{c} \cap \{|x_{j}|\le 1 \}}  |x_{j}| \sup_{s \in [0,h]}\left|e^{isx_{j}}-1\right|   \nu(dx) + \lim_{h \to 0}\int_{D^{c} \cap \{|x_{j}|>1 \}}  |x_{j}| \sup_{s \in [0,h]}\left|e^{isx_{j}}-1\right|   \nu(dx)                        \\
         & \le  \lim_{h \to 0}\int_{D^{c} \cap \{|x_{j}|\le 1 \}}  \sup_{s \in [0,h]}\left|e^{isx_{j}}-1\right|   \nu(dx) + \lim_{h \to 0}\int_{D^{c} \cap \{|x_{j}|>1 \}}  |x_{j}| \sup_{s \in [0,h]}\left|e^{isx_{j}}-1\right|   \nu(dx)         \label{line:2lim 2 int} \\
         & \le 0
    \end{align}
    }
    On line (\ref{line:2lim 2 int}) the integrals are dominated by $\int_{D^{c}} 2 \nu(dx), \int_{|x_{j}|>1 }2 |x_{j}| \nu(dx)  < \infty $ and the dominated convergence theorem can be applied. After exchanging both limits are $0$. \\
    $\partial z_{j} \partial z_{k}|_{z=0}I(z) = \int_{\mathbb{R}^{d}}  \partial z_{j} \partial z_{k} |_{z=0}\operatorname{e x p} ( i \langle z, x \rangle) \nu( d x )
        = -\int_{\mathbb{R}^{d}} x_{j} x_{k} \nu( d x )$ where exchanging the integral and the derivative isn't  elaborated.
\end{proof}

\printbibliography

\end{document}